{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the necessary libraries and packages.NLTK,SCIKIT-LEARN,Other python Libraries have to be installed on the system to proceed any further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import csv\n",
    "import math\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy import sparse,io\n",
    "from scipy.sparse import csr_matrix,lil_matrix\n",
    "from operator import itemgetter\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are grouping all the fileids(Document ID) in the corpora into the list fileids. \n",
    "\n",
    "The sequence of grouping these fileids has significance file retrieving the Document details at the end.In the cell below, few of the corpora were not identified, so retrieved from them using CORPUS ROOT.\n",
    "\n",
    "For this cell to run, this project folder has to be in home directory,with nltk_data folder in home directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16356\n"
     ]
    }
   ],
   "source": [
    "fileids =[]\n",
    "\n",
    "fileids = fileids + ['reuters@'+fd for fd in nltk.corpus.reuters.fileids()]\n",
    "fileids = fileids + ['abc@'+fd for fd in nltk.corpus.abc.fileids()]\n",
    "fileids = fileids + ['webtext@'+fd for fd in nltk.corpus.webtext.fileids()]\n",
    "fileids = fileids + ['comparative_sentences@'+fd for fd in nltk.corpus.comparative_sentences.fileids()]\n",
    "fileids = fileids + ['names@'+fd for fd in nltk.corpus.names.fileids()]\n",
    "fileids = fileids + ['alpino@'+fd for fd in nltk.corpus.alpino.fileids()]\n",
    "fileids = fileids + ['brown@'+fd for fd in nltk.corpus.brown.fileids()]\n",
    "fileids = fileids + ['cess_cat@'+fd for fd in nltk.corpus.cess_cat.fileids()]\n",
    "fileids = fileids + ['gutenberg@'+fd for fd in nltk.corpus.gutenberg.fileids()]\n",
    "\n",
    "fileids = fileids + ['genesis@'+fd for fd in nltk.corpus.genesis.fileids()]\n",
    "fileids = fileids + ['ieer@'+fd for fd in nltk.corpus.ieer.fileids()]\n",
    "fileids = fileids + ['cess_esp@'+fd for fd in nltk.corpus.cess_esp.fileids()]\n",
    "fileids = fileids + ['shakespeare@'+fd for fd in nltk.corpus.shakespeare.fileids()]\n",
    "fileids = fileids + ['nps_chat@'+fd for fd in nltk.corpus.nps_chat.fileids()]\n",
    "fileids = fileids + ['state_union@'+fd for fd in nltk.corpus.state_union.fileids()]\n",
    "\n",
    "fileids = fileids + ['movie_reviews@'+fd for fd in nltk.corpus.movie_reviews.fileids()]\n",
    "fileids = fileids + ['product_reviews_1@'+fd for fd in nltk.corpus.product_reviews_1.fileids()]\n",
    "fileids = fileids + ['product_reviews_2@'+fd for fd in nltk.corpus.product_reviews_2.fileids()]\n",
    "fileids = fileids + ['conll2000@'+fd for fd in nltk.corpus.conll2000.fileids()]\n",
    "fileids = fileids + ['conll2002@'+fd for fd in nltk.corpus.conll2002.fileids()]\n",
    "fileids = fileids + ['twitter_samples@'+fd for fd in nltk.corpus.twitter_samples.fileids()]\n",
    "fileids = fileids + ['dependency_treebank@'+fd for fd in nltk.corpus.dependency_treebank.fileids()]\n",
    "fileids = fileids + ['rte@'+fd for fd in nltk.corpus.rte.fileids()]\n",
    "fileids = fileids + ['inaugural@'+fd for fd in nltk.corpus.inaugural.fileids()]\n",
    "\n",
    "corpus_root = '../nltk_data/corpora/pe08'\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '.*')\n",
    "\n",
    "fileids = fileids + ['pe08@'+fd for fd in wordlists.fileids()]\n",
    "\n",
    "corpus_root = '../nltk_data/corpora/europarl_raw'\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '.*')\n",
    "\n",
    "fileids = fileids + ['europarl_raw@'+fd for fd in wordlists.fileids()]\n",
    "\n",
    "corpus_root = '../nltk_data/corpora/chat80'\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '.*')\n",
    "\n",
    "fileids = fileids + ['chat80@'+fd for fd in wordlists.fileids()]\n",
    "\n",
    "corpus_root = '../nltk_data/corpora/brown_tei'\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '.*')\n",
    "\n",
    "fileids = fileids + ['brown_tei@'+fd for fd in wordlists.fileids()]\n",
    "print(len(fileids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here Corpus is a list which takes raw data from the files.\n",
    "\n",
    "Even here the sequence in which the files considered for retrieving raw data has to be same with the sequence of the fileids in the above cell.\n",
    "\n",
    "Reason - During final retrieval we just get a number like (15345) as document number, which then looked up in fileid list gives [Corresponding_Corpus@file].\n",
    "\n",
    "Even here few of the corpus are considered through path.\n",
    "\n",
    "Outputs total files considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16356\n"
     ]
    }
   ],
   "source": [
    "Corpus=[]\n",
    "Corpus = Corpus + [reuters.raw(fd) for fd in reuters.fileids()]\n",
    "Corpus = Corpus + [abc.raw(fd) for fd in abc.fileids()]\n",
    "Corpus = Corpus + [webtext.raw(fd) for fd in webtext.fileids()]\n",
    "Corpus = Corpus + [comparative_sentences.raw(fd) for fd in comparative_sentences.fileids()]\n",
    "Corpus = Corpus + [names.raw(fd) for fd in names.fileids()]\n",
    "Corpus = Corpus + [alpino.raw(fd) for fd in alpino.fileids()]\n",
    "Corpus = Corpus + [brown.raw(fd) for fd in brown.fileids()]\n",
    "Corpus = Corpus + [cess_cat.raw(fd) for fd in cess_cat.fileids()]\n",
    "Corpus = Corpus + [gutenberg.raw(fd) for fd in gutenberg.fileids()]\n",
    "Corpus = Corpus + [genesis.raw(fd) for fd in genesis.fileids()]\n",
    "Corpus = Corpus + [ieer.raw(fd) for fd in ieer.fileids()]\n",
    "Corpus = Corpus + [cess_esp.raw(fd) for fd in cess_esp.fileids()]\n",
    "Corpus = Corpus + [shakespeare.raw(fd) for fd in shakespeare.fileids()]\n",
    "Corpus = Corpus + [nps_chat.raw(fd) for fd in nps_chat.fileids()]\n",
    "Corpus = Corpus + [state_union.raw(fd) for fd in state_union.fileids()]\n",
    "\n",
    "\n",
    "Corpus = Corpus + [movie_reviews.raw(fd) for fd in movie_reviews.fileids()]\n",
    "Corpus = Corpus + [product_reviews_1.raw(fd) for fd in product_reviews_1.fileids()]\n",
    "Corpus = Corpus + [product_reviews_2.raw(fd) for fd in product_reviews_2.fileids()]\n",
    "Corpus = Corpus + [conll2000.raw(fd) for fd in conll2000.fileids()]\n",
    "Corpus = Corpus + [conll2002.raw(fd) for fd in conll2002.fileids()]\n",
    "Corpus = Corpus + [twitter_samples.raw(fd) for fd in twitter_samples.fileids()]\n",
    "Corpus = Corpus + [dependency_treebank.raw(fd) for fd in dependency_treebank.fileids()]\n",
    "Corpus = Corpus + [rte.raw(fd) for fd in rte.fileids()]\n",
    "Corpus = Corpus + [inaugural.raw(fd) for fd in inaugural.fileids()]\n",
    "\n",
    "corpus_root = '../nltk_data/corpora/pe08'\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '.*')\n",
    "\n",
    "Corpus = Corpus + [wordlists.raw(fd) for fd in wordlists.fileids()]\n",
    "\n",
    "corpus_root = '../nltk_data/corpora/europarl_raw'\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '.*')\n",
    "\n",
    "Corpus = Corpus + [wordlists.raw(fd) for fd in wordlists.fileids()]\n",
    "\n",
    "corpus_root = '../nltk_data/corpora/chat80'\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '.*')\n",
    "\n",
    "Corpus = Corpus + [wordlists.raw(fd) for fd in wordlists.fileids()]\n",
    "\n",
    "corpus_root = '../nltk_data/corpora/brown_tei'\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '.*')\n",
    "\n",
    "Corpus = Corpus + [wordlists.raw(fd) for fd in wordlists.fileids()]\n",
    "print(len(Corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building of Term Document Matrix manually can't be done with such a large data set, as it is very inefficient and throws MEMORY ERROR while trying to traverse the matrix.\n",
    "\n",
    "So we wanted to use Sparse matrix for all the matrices\n",
    "1) Term Document Matrix\n",
    "2) Inverse Document Frequency Matrix\n",
    "3) Tf-Idf Matrix\n",
    "4) Normaised Tf-Idf Matrix\n",
    "\n",
    "countVectorizer() of sklearn takes in raw data( Corpus we have built in previous cell), and returns Scipy Compressed Sparse Matrix which is our Term Document Matrix.\n",
    "\n",
    "Rows - rows are the plain numbers each corresponding to a fileid, this is why we have to keep track the sequence in which raw data of files(Corpus) is fed in to CountVectorizer.\n",
    "\n",
    "Tokenization - CountVectorizer tokenizes the raw data based on parameters spaces and punctuation marks.\n",
    "\n",
    "Stemming - analyzer attribute of countVectorizer provides option for preprocessing.Here we take in all the tokens and   stem these tokens using nltk.PorterStemmer()..check stemmed_words() definition in below cell for details.\n",
    "\n",
    "Columns - Columns are the stemmed tokens after getting rid of redundancy and Stopwords.\n",
    "\n",
    "WE CHOSE TO STEM AS WE TARGETED HIGH RECALL. IF PRECISION WAS THE GOAL WE WOULD HAVE GONE WITH LEMMATIZATION INSTEAD OF STEMMING."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "porter  = nltk.PorterStemmer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (porter.stem(w) for w in analyzer(doc) if w.isdigit() is False and w.isalpha() is True)\n",
    "\n",
    "stem_vectorizer = CountVectorizer(stop_words='english',analyzer=stemmed_words)\n",
    "X = stem_vectorizer.fit_transform(Corpus) \n",
    "#print(X)\n",
    "type(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell takes lot of time and is a an overhead on CPU, so we chose to store the sparse matrix built in above cell and store it in a file \"TermDocStemmed.mtx\" that comes in this folder.\n",
    "\n",
    "In below cell we write the sparse matrix(Term Document Matrix) X to the mentioned file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "io.mmwrite(\"TermDocStemmed.mtx\",X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need column names of the (Term Document Matrix) to map the normalised tokens of the query string inorder to build Query Vector. \n",
    "\n",
    "In below cell we write the features(Column names) to csv file so that we dont have to run this cell later.Just reading voc(list) would suffice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400737\n"
     ]
    }
   ],
   "source": [
    "voc = stem_vectorizer.get_feature_names() # voc is the list of features(Column names)\n",
    "print(len(voc))\n",
    "with open('Vocabulary.csv','w') as handle:\n",
    "    wr = csv.writer(handle, dialect='excel')\n",
    "    wr.writerows(voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, in below cell we read voc(list) - list of feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a' 'aa' 'aaa' ..., 'ώριμος' 'ώς' 'ώστε']\n"
     ]
    }
   ],
   "source": [
    "voc = []\n",
    "with open('Vocabulary.csv','r') as handle:\n",
    "    reader = csv.reader(handle,delimiter = ',')\n",
    "    for row in reader:\n",
    "        voc.append(row)\n",
    "voc = np.array(voc)\n",
    "for i in range(len(voc)):\n",
    "    voc[i] = ''.join(voc[i])\n",
    "print(voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In below cell we read the TermDoc Matrix into Y.\n",
    "Tf = if Term Frequency!=0 (1 + log(Term Frequency)) else 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = io.mmread(\"TermDocStemmed.mtx\")\n",
    "Y = Y.tocsr()       # Sparse matrix Y converted to Compressed Sparse Row Matrix,which is efficient with arithmetic operations. \n",
    "if Y.data is not 0:  # if the term is 0, it remains 0.\n",
    "    Y.data = np.log10(Y.data) # applying log() at matrix level\n",
    "    Y.data = Y.data + 1 # adding 1 at matrix level to data terms satisfying if clause mentioned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 27, 17, ...,  1,  2, 10], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TotalFiles = 16356 # obtained through len(fileids)\n",
    "DocFreq = np.diff(Y.tocsc().indptr) # Here we efficiently calculate no. of non zero terms in each column.Which is\n",
    "DocFreq                             # Document Frequency of a word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse Document Frequency Matrix matrix building procedure:\n",
    "1) Array of length equal to numbers of features(tokens) each equal to N(Total Documents).\n",
    "2) Building Document Frequency array of all the features\n",
    "3) Divide Array1 by array of Document Frequency(Numpy does this elementwise).\n",
    "4) Then apply log() on the above array to get Inverse Document Frequency array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 16356.        ,    605.77777778,    962.11764706, ...,\n",
       "        16356.        ,   8178.        ,   1635.6       ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = np.array([16356 for i in range(400737)]) #Building \n",
    "m1 = m1/DocFreq\n",
    "m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.2136771 ,  2.78231334,  2.98322818, ...,  4.2136771 ,\n",
       "        3.91264711,  3.2136771 ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InvDocFreq = np.log10(m1)\n",
    "InvDocFreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Pictures/Selection_003.bmp\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16356, 400737)\n"
     ]
    }
   ],
   "source": [
    "d = lil_matrix((400737,400737)) # Using lil_matrix of SciPy,large sparse array is built very fast. \n",
    "d.setdiag(InvDocFreq) # Converting InvDocFreq(row matrix) to a Diagonal Matrix.\n",
    "Tf_Idf_Matrix = Y*d   # here we get tf-idf matrix.\n",
    "print(Tf_Idf_Matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to carry out normalisation of tf-idf matrix.\n",
    "Normalisation for document vectors while calculating cosine similarity is carried out here and each row representing a Document vector is divided by its corresponding Eucledian distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "squared_Tf_Idf = Tf_Idf_Matrix.copy()\n",
    "squared_Tf_Idf.data **= 2\n",
    "squared_sum = squared_Tf_Idf.sum(1) # Here summing the squares of all elements of a row row-wise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we normalise tf-idf matrix applying previous technique for row-wise multiplication.But here we have to divide ,which  is done by taking reciprocal of elements of euclid_dis and multiplying the way we did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.lil.lil_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16356, 400737)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "euclid_dis = np.squeeze(np.array(squared_sum)) # flattening squared_sum to numpy array of squared distences.\n",
    "euclid_dis = [1/i for i in squared_sum if i!=0]\n",
    "euclid_dis = np.sqrt(euclid_dis)\n",
    "len(euclid_dis)\n",
    "d = lil_matrix((16356,16356))\n",
    "d.setdiag(euclid_dis)\n",
    "print(type(d))\n",
    "Tf_Idf_Matrix_Trans = Tf_Idf_Matrix.transpose()\n",
    "Trans_Normalised_Tf_Idf_Matrix = Tf_Idf_Matrix_Trans*d\n",
    "Normalised_Tf_Idf_Matrix = Trans_Normalised_Tf_Idf_Matrix.transpose()\n",
    "Normalised_Tf_Idf_Matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HERE IN BELOW CELL WE TAKE QUERY.\n",
    "1) Convert to lower case\n",
    "2) Get rid of stop words.\n",
    "3) Split the query at spaces and then STEM them,only then they match with the original stemmed vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = input(\"Make a query: \")\n",
    "qry = query.split()\n",
    "qry = [w.lower() for w in qry if w.lower() not in nltk.corpus.stopwords.words('english')]\n",
    "porter  = nltk.PorterStemmer()\n",
    "qry = [porter.stem(w) for w in qry]\n",
    "qry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here in cell below, we build query vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qry_vector = [0 for i in range(400737)]\n",
    "for q in qry:\n",
    "    for i,j in enumerate(voc):\n",
    "        if j==q:\n",
    "            qry_vector[i] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cell below, Normalising query vector, dividing by Eucledian distance  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400737"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqr = 0\n",
    "for i in qry_vector:\n",
    "    if i is not 0:\n",
    "        sqr += i*i\n",
    "euclid_dist = math.sqrt(sqr)\n",
    "if euclid_dist != 0:\n",
    "    for i in qry_vector:\n",
    "        i = i/euclid_dist\n",
    "len(qry_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we row-wise multiply query vector with Document vectors in Normalised_Tf_Idf_Matrix to obtain Dot_Product_Matrix.\n",
    "Procedure for doing this is presented earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = lil_matrix((400737,400737))\n",
    "d.setdiag(qry_vector)\n",
    "Dot_Product_Matrix = Normalised_Tf_Idf_Matrix*d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot product values (Cosine Similarities) obtained by adding elements of Dot_Product_val along the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Dot_Product_val = Dot_Product_Matrix.sum(1)\n",
    "Dot_Product_val = np.squeeze(np.array(Dot_Product_val))\n",
    "#Dot_Product_val[13447]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cell below, what we have essentially done is collected all the cosine similarity values which are not zero along with there row numbers, as row numbers are the face of fileids. \n",
    "\n",
    "So used list of tuples.Tuple Structure -----> (Row number representative of fileid, cosine similarity)\n",
    "\n",
    "In final list we return top 10 results(by sorting tuples based on cosine similarity),if less than 10 results return all,\n",
    "if no results---->print(0 matches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(15441, 0.56614839156701802), (14253, 0.35437012896847087), (14753, 0.32469913930898242), (15314, 0.32249014214499927), (14690, 0.29676066457960781), (14943, 0.26343863823621472), (14928, 0.25519065155958831), (14054, 0.2407511612879758), (14816, 0.20108643409438764), (15143, 0.19917586850382918)]\n"
     ]
    }
   ],
   "source": [
    "Dot_Product_val_list = [] # list of tuples whose cosine similarity!=0\n",
    "Final_list = []    # list in which we return top 10 results.\n",
    "for i in range(16356):\n",
    "    if Dot_Product_val[i]!=0 :\n",
    "        Dot_Product_val_list = Dot_Product_val_list + [(i,Dot_Product_val[i])]\n",
    "Dot_Product_val_list = sorted(Dot_Product_val_list,key=lambda x: x[1],reverse=True)\n",
    "\n",
    "if(len(Dot_Product_val_list)==0):\n",
    "    print(\"Sorry. No matches found!!!\")\n",
    "elif(len(Dot_Product_val_list)<=10):\n",
    "    Final_list = Dot_Product_val_list\n",
    "else:\n",
    "    Final_list = Dot_Product_val_list[:10]\n",
    "print(Final_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the first element of the tuples above is a mere number.\n",
    "Remember that Doc ID's in fileids list are considered in same sequence as considering raw-data of various file by fileids.\n",
    "Explanation:\n",
    "\n",
    "Corpus[123] is the raw data of fileids[123]. As data in DocFreq matrix goes in the sequence of raw data we give.\n",
    "Row number 123 corresponds to fileids[123].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['twitter_samples@tweets.20150430-223406.json',\n",
       " 'movie_reviews@neg/cv838_25886.txt',\n",
       " 'movie_reviews@pos/cv338_8821.txt',\n",
       " 'movie_reviews@pos/cv899_16014.txt',\n",
       " 'movie_reviews@pos/cv275_28887.txt',\n",
       " 'movie_reviews@pos/cv528_10822.txt',\n",
       " 'movie_reviews@pos/cv513_6923.txt',\n",
       " 'movie_reviews@neg/cv639_10797.txt',\n",
       " 'movie_reviews@pos/cv401_12605.txt',\n",
       " 'movie_reviews@pos/cv728_16133.txt']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_file_id_list = [] # list of specific_corpus@fileid.\n",
    "for (i,j) in Final_list:\n",
    "    Final_file_id_list = Final_file_id_list + [fileids[i]]\n",
    "Final_file_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twitter_samples tweets.20150430-223406.json\n",
      "movie_reviews neg/cv838_25886.txt\n",
      "movie_reviews pos/cv338_8821.txt\n",
      "movie_reviews pos/cv899_16014.txt\n",
      "movie_reviews pos/cv275_28887.txt\n",
      "movie_reviews pos/cv528_10822.txt\n",
      "movie_reviews pos/cv513_6923.txt\n",
      "movie_reviews neg/cv639_10797.txt\n",
      "movie_reviews pos/cv401_12605.txt\n",
      "movie_reviews pos/cv728_16133.txt\n"
     ]
    }
   ],
   "source": [
    "for v in Final_file_id_list:\n",
    "    actual_corpus,file = v.split('@',1)\n",
    "    print(actual_corpus,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = twitter_samples.strings('positive_tweets.json')\n",
    "a = [i.split() for i in a ]\n",
    "a = np.array(a)\n",
    "a = np.ravel(a)\n",
    "\n",
    "for i in a:\n",
    "    for j in i:\n",
    "        if j=='':\n",
    "            print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As both actual_corpus and fileid come as output, relevance of result can be checked as done above\n",
    "a = nltk.corpus.actual_corpus.fileid.\n",
    "print(a[:100]) lets us no how relevant a result is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toestand van het gras is te lamentabel\n",
      "in slaagt het gras te laten voldoen\n",
      "na estafettetocht Het gras komt van de\n",
      "sporen in het gras , waarna met\n",
      "je rustig het gras af kon maaien\n",
      "addertje onder het gras , geven zowel\n",
      "hij zich op gras uitstekend in zijn\n",
      "zijn op het gras van Wimbledon in\n",
      "op de riz gras , de West-Afrikaanse\n"
     ]
    }
   ],
   "source": [
    "a = conll2002.words()\n",
    "for j,i in enumerate(a):\n",
    "    if porter.stem(i)=='gra':\n",
    "        print (a[j-3],a[j-2],a[j-1],a[j],a[j+1],a[j+2],a[j+3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'enchantment'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
